{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\n\nfrom tensorflow.keras.models import Sequential \nfrom keras.layers import Dense, Dropout, Flatten,Conv2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam,RMSprop,SGD\nfrom keras.layers import MaxPooling2D\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator,load_img\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping, ReduceLROnPlateau\nfrom keras.regularizers import l2\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:20:27.738472Z","iopub.execute_input":"2021-11-26T09:20:27.738809Z","iopub.status.idle":"2021-11-26T09:20:33.318387Z","shell.execute_reply.started":"2021-11-26T09:20:27.738766Z","shell.execute_reply":"2021-11-26T09:20:33.317635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_train = \"../input/fer2013/train\"\ndir_test = \"../input/fer2013/test\"","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:20:33.319963Z","iopub.execute_input":"2021-11-26T09:20:33.320216Z","iopub.status.idle":"2021-11-26T09:20:33.326604Z","shell.execute_reply.started":"2021-11-26T09:20:33.320182Z","shell.execute_reply":"2021-11-26T09:20:33.325745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalization: Scaling data to the range of 0-1 is traditionally referred to as normalization.\ntrain_data_gen = ImageDataGenerator(\n    rescale=1.0/255,\n    horizontal_flip = True,\n    zoom_range=0.2\n)\n\ntest_data_gen = ImageDataGenerator(\n    rescale=1.0/255,\n    validation_split = 0.2\n)\n\n\n# Making train data from the set of images.\ntrain_data = train_data_gen.flow_from_directory(\n    directory=dir_train,\n    target_size= (48,48), #Change later to find the values.\n    batch_size=48,\n    color_mode='grayscale',\n    class_mode='categorical'\n)\n# Making test data from the set of images.\ntest_data = train_data_gen.flow_from_directory(\n    directory=dir_test,\n    target_size= (48,48), #Change later to find the values.\n    batch_size=48,\n    color_mode='grayscale',\n    class_mode='categorical'\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:20:33.328149Z","iopub.execute_input":"2021-11-26T09:20:33.328571Z","iopub.status.idle":"2021-11-26T09:20:47.356693Z","shell.execute_reply.started":"2021-11-26T09:20:33.328529Z","shell.execute_reply":"2021-11-26T09:20:47.355914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,22))\ncounter = 1\nfor expression in os.listdir(dir_train):\n    img = load_img((dir_train+\"/\" + expression +'/'+ os.listdir(dir_train +\"/\"+ expression)[1]))\n    plt.subplot(1,7,counter)\n    plt.imshow(img)\n    plt.title(expression)\n    counter += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:20:47.358728Z","iopub.execute_input":"2021-11-26T09:20:47.359514Z","iopub.status.idle":"2021-11-26T09:20:48.247064Z","shell.execute_reply.started":"2021-11-26T09:20:47.359474Z","shell.execute_reply":"2021-11-26T09:20:48.246344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n\n        # input layer\n        Conv2D(filters=32,\n               kernel_size=(3,3),\n               padding='same',\n               activation = 'relu',\n               input_shape=(48,48,1)),\n        Conv2D(filters=64,\n               kernel_size=(3,3),\n               padding='same', \n               activation='relu'),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n\n        #hidden layers\n        # 1st layers\n        Conv2D(filters=128,\n               kernel_size=(3,3), \n               padding='same', \n               activation='relu',\n               kernel_regularizer=l2(0.001)),\n    \n        # 2nd layer\n        Conv2D(filters=256, \n               kernel_size=(3,3), \n               padding='same', \n               activation='relu',\n               kernel_regularizer=l2(0.001)),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n    \n     # 3rd layer\n        Conv2D(filters=512, \n               kernel_size=(3,3), \n               padding='same', \n               activation='relu',\n               kernel_regularizer=l2(0.001)),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n    \n        # Flattenning the layer and passing making NN\n        Flatten(),\n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.25),\n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.25),\n    \n    \n    \n        # output layer\n        Dense(7, activation='softmax')\n    ])\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(learning_rate=0.0005),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:20:48.248414Z","iopub.execute_input":"2021-11-26T09:20:48.248774Z","iopub.status.idle":"2021-11-26T09:20:50.816373Z","shell.execute_reply.started":"2021-11-26T09:20:48.248731Z","shell.execute_reply":"2021-11-26T09:20:50.815612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Summary","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:20:50.817672Z","iopub.execute_input":"2021-11-26T09:20:50.8181Z","iopub.status.idle":"2021-11-26T09:20:50.832992Z","shell.execute_reply.started":"2021-11-26T09:20:50.818062Z","shell.execute_reply":"2021-11-26T09:20:50.832357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Diagram\nref:: https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/","metadata":{}},{"cell_type":"code","source":"plot_model(model, to_file='cnn_model.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:20:50.834644Z","iopub.execute_input":"2021-11-26T09:20:50.834995Z","iopub.status.idle":"2021-11-26T09:20:52.194708Z","shell.execute_reply.started":"2021-11-26T09:20:50.834956Z","shell.execute_reply":"2021-11-26T09:20:52.193921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CallBacks","metadata":{}},{"cell_type":"markdown","source":"1. Checkpoints that monitors the validation loss and save the loss to given file path.\n2. EarlyStoping with respect to val_accuracy, if the accuracy remains same for more than 15 time, we are stopping\n3. ReduceLROnPlateau to reduce the LR by 0.2 if it get stuck on the plateau.","metadata":{}},{"cell_type":"code","source":"filepath = \"emotion-detector.hdf5\"\ncheckpoint = ModelCheckpoint(filepath,\n                             monitor=\"val_accuracy\",\n                             verbose=1,\n                             save_best_only=True,\n                             mode=\"max\")\n\nearlystop = EarlyStopping(monitor='val_accuracy',\n                          verbose=1, \n                          min_delta=0, \n                          patience=15, \n                          restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              verbose=1,\n                              factor=0.2, \n                              patience=6, \n                              min_delta=0.0001)\n\n\ncallbacks = [checkpoint, earlystop, reduce_lr]","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:20:52.196101Z","iopub.execute_input":"2021-11-26T09:20:52.196371Z","iopub.status.idle":"2021-11-26T09:20:52.212341Z","shell.execute_reply.started":"2021-11-26T09:20:52.196335Z","shell.execute_reply":"2021-11-26T09:20:52.208914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion_model_info = model.fit(\n        train_data,\n        steps_per_epoch=train_data.samples // 64,\n        epochs=100,\n        validation_data=test_data,\n        validation_steps=test_data.samples // 63,\n        callbacks=callbacks)\nmodel.save_weights(\"emotion_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T09:20:52.216084Z","iopub.execute_input":"2021-11-26T09:20:52.219074Z","iopub.status.idle":"2021-11-26T10:18:50.365373Z","shell.execute_reply.started":"2021-11-26T09:20:52.219032Z","shell.execute_reply":"2021-11-26T10:18:50.363901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting val_acc and acc with respect with epocs","metadata":{}},{"cell_type":"code","source":"model.save_weights('model.h5')\nprint(emotion_model_info.history.keys())\nplt.plot(range(92),emotion_model_info.history[\"accuracy\"],label=\"train\")\nplt.plot(range(92),emotion_model_info.history[\"val_accuracy\"],label=\"validation\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Train and Valid Accuracy over the epochs\",size=15)\nplt.savefig('acc_val_acc.png', transparent=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:32:28.638309Z","iopub.execute_input":"2021-11-26T10:32:28.638571Z","iopub.status.idle":"2021-11-26T10:32:28.987708Z","shell.execute_reply.started":"2021-11-26T10:32:28.638541Z","shell.execute_reply":"2021-11-26T10:32:28.986008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting val_loss and loss with respect with epocs","metadata":{}},{"cell_type":"code","source":"print(emotion_model_info.history.keys())\nplt.plot(range(92),emotion_model_info.history[\"val_loss\"],label=\"validation\")\nplt.plot(range(92),emotion_model_info.history[\"loss\"],label=\"train\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Train and Valid Loss over the epochs\",size=15)\nplt.savefig('val_loss_vs_loss.png', transparent=True)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:32:32.33901Z","iopub.execute_input":"2021-11-26T10:32:32.339534Z","iopub.status.idle":"2021-11-26T10:32:32.609648Z","shell.execute_reply.started":"2021-11-26T10:32:32.339496Z","shell.execute_reply":"2021-11-26T10:32:32.608025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evalutaion of the model.","metadata":{}},{"cell_type":"code","source":"train_loss, train_accu = model.evaluate(train_data)\ntest_loss, test_accu = model.evaluate(test_data)\nprint(\"Final training accuracy = {:.2f} , validation accuracy = {:.2f}\".format(train_accu*100, test_accu*100))\nprint(\"Final training loss = {:.2f} , validation loss = {:.2f}\".format(train_loss, test_loss))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:32:36.400683Z","iopub.execute_input":"2021-11-26T10:32:36.401231Z","iopub.status.idle":"2021-11-26T10:33:17.35796Z","shell.execute_reply.started":"2021-11-26T10:32:36.401191Z","shell.execute_reply":"2021-11-26T10:33:17.357167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Realtime face emotion detection using opencv","metadata":{}},{"cell_type":"code","source":"# define a video capture object\nemotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\nvid = cv2.VideoCapture(-2)\nwhile(True):\n\n    # Capture the video frame\n    # by frame\n    ret, frame = vid.read()\n\n    if not ret:\n        break\n\n    face_region = cv2.CascadeClassifier(\n        cv2.data.haarcascades+\n        \"haarcascade_frontalface_default.xml\"\n    )\n    # converting to grayscale, as IT take value in opposite direction BGR, using grayscale\n    # make it easy to compute as it only has one channel only.\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    faces = face_region.detectMultiScale(\n        gray, scaleFactor=1.3, minNeighbors=5)\n\n    if faces == ():\n        print(\"No faces found\")\n\n    for (x, y, w, h) in faces:\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (127, 0, 255), 2)\n        roi_gray = gray[y:y+h, x:x+w]\n        cropped_img = np.expand_dims(np.expand_dims(\n            cv2.resize(roi_gray, (48, 48)), -1), 0)\n        prediction = model.predict(cropped_img)\n        max_index = int(np.argmax(prediction))\n        cv2.putText(frame, emotion_dict[max_index], (x+15, y+20),\n                    cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (127, 0, 255), 2, cv2.LINE_4)\n    # Display the resulting frameq\n    cv2.imshow(\"frame\", frame)\n    # the 'q' button is set as the\n    # quitting button you may use any\n    # desired button of your choice\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# After the loop release the cap object\nvid.release()\n# Destroy all the windows\ncv2.destroyAllWindows()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:18:50.953561Z","iopub.status.idle":"2021-11-26T10:18:50.954127Z","shell.execute_reply.started":"2021-11-26T10:18:50.953888Z","shell.execute_reply":"2021-11-26T10:18:50.953915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(test_data)\ny_pred = np.argmax(y_pred, axis=1)\nclass_labels = test_data.class_indices\nclass_labels = {v:k for k,v in class_labels.items()}\n\n#from sklearn.metrics import classification_report, confusion_matrix\ncm_test = confusion_matrix(test_data.classes, y_pred)\ncm_test = cm_test / cm_test.sum(axis=1)[:, np.newaxis]\n\nprint('Confusion Matrix')\nprint(cm_test)\nprint('Classification Report')\ntarget_names = list(class_labels.values())\nprint(classification_report(test_data.classes, y_pred, target_names=target_names))\n\nplt.figure(figsize=(8,8))\nplt.imshow(cm_test, interpolation='nearest')\nplt.colorbar()\ntick_mark = np.arange(len(target_names))\n_ = plt.xticks(tick_mark, target_names, rotation=90)\n_ = plt.yticks(tick_mark, target_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T10:34:12.427868Z","iopub.execute_input":"2021-11-26T10:34:12.42888Z","iopub.status.idle":"2021-11-26T10:34:20.158558Z","shell.execute_reply.started":"2021-11-26T10:34:12.42881Z","shell.execute_reply":"2021-11-26T10:34:20.157732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediting images from the test data-set","metadata":{}},{"cell_type":"code","source":"for expression in os.listdir(dir_train):\n    img = image.load_img((dir_train+\"/\" + expression +'/'+ os.listdir(dir_train +\"/\"+ expression)[1]),target_size = (48,48),color_mode = \"grayscale\")\n    img = np.array(img)\n    plt.imshow(img)\n    print(img.shape)\n\n    img = np.expand_dims(img,axis = 0) #makes image shape (1,48,48)\n    img = img.reshape(1,48,48,1)\n    result = model.predict(img)\n    result = list(result[0])\n    img_index = result.index(max(result))\n    print(emotion_dict[img_index])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T11:02:37.793642Z","iopub.execute_input":"2021-11-26T11:02:37.794461Z","iopub.status.idle":"2021-11-26T11:02:37.871808Z","shell.execute_reply.started":"2021-11-26T11:02:37.794339Z","shell.execute_reply":"2021-11-26T11:02:37.870604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}