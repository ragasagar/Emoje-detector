{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74cf0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten,Conv2D\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Normalization: Scaling data to the range of 0-1 is traditionally referred to as normalization.\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    ")\n",
    "\n",
    "test_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    ")\n",
    "\n",
    "\n",
    "# Making train data from the set of images.\n",
    "train_data = train_data_gen.flow_from_directory(\n",
    "    directory='data-set/train',\n",
    "    target_size= (48,48), #Change later to find the values.\n",
    "    batch_size=64,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical'\n",
    ")\n",
    "# Making test data from the set of images.\n",
    "test_data = train_data_gen.flow_from_directory(\n",
    "    directory='data-set/test',\n",
    "    target_size= (48,48), #Change later to find the values.\n",
    "    batch_size=64,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(4, 4),\n",
    "    strides=(1, 1),  # (1,1) defines strides for width, and height\n",
    "    input_shape=(48, 48, 1),\n",
    "    padding='valid',\n",
    "    activation='relu'\n",
    "))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "168/224 [=====================>........] - ETA: 29s - loss: 1.8271 - accuracy: 0.2478"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(\n",
    "    learning_rate=0.00001, decay=1e-6), metrics=['accuracy'])\n",
    "model.fit(train_data, steps_per_epoch=train_data.samples//128,\n",
    "                    epochs=50, validation_data=test_data, validation_steps=test_data.samples//128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a video capture object\n",
    "vid = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "\n",
    "    # Capture the video frame\n",
    "    # by frame\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    face_region = cv2.CascadeClassifier(\n",
    "        \"haarcascade_frontalface_default.xml\"\n",
    "    )\n",
    "    # converting to grayscale, as IT take value in opposite direction BGR, using grayscale\n",
    "    # make it easy to compute as it only has one channel only.\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_region.detectMultiScale(\n",
    "        gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    if faces == ():\n",
    "        print(\"No faces found\")\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (127, 0, 255), 2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(\n",
    "            cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        prediction = model.predict(cropped_img)\n",
    "        max_index = int(np.argmax(prediction))\n",
    "        cv2.putText(frame, emotion_dict[max_index], (x+15, y+20),\n",
    "                    cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (127, 0, 255), 2, cv2.LINE_4)\n",
    "    # Display the resulting frameq\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    # the 'q' button is set as the\n",
    "    # quitting button you may use any\n",
    "    # desired button of your choice\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# After the loop release the cap object\n",
    "vid.release()\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
